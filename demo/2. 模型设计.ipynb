{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、声学模型设计\n",
    "本实验借鉴了视觉的经典网络结构，声学模型采用VGGNet的多个卷积层。  \n",
    "本部分代码在 ```acoustic_model.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. VGGNet简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGNet是由牛津大学计算机视觉组合和Google DeepMind公司研究员一起研发的深度卷积神经网络。  \n",
    "它通过反复的堆叠小型卷积核和最大池化层，成功的构建了16~19层深的卷积神经网络。  \n",
    "VGGNet获得了ILSVRC2014年比赛的亚军和定位项目的冠军。目前为止，VGGNet依然被用来提取图像的特征。\n"
   ]
  },
  {
   "source": [
    "### 2. VGGNet网络结构\n",
    "VGG16的网络结构如下图所示：  \n",
    "其中224x224x3的彩色图表示3通道的长和宽都为224的图像数据，也是网络的输入层；  \n",
    "白色部分为卷积层，红色部分为池化层（使用最大池化），蓝色部分为全连接层，其中卷积层和全连接层的激活函数都使用relu；  \n",
    "总的来说，VGG16网络为13层卷积层+5层池化层+3层全连接层而组成。  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"./png/vggnet1.png\"/></div>"
   ]
  },
  {
   "source": [
    "### 3. 声学模型网络结构\n",
    "借鉴VGGNet，我们设计的网络结构如下图所示：  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 自定义模型训练损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**思路简介**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：使用函数```get_pooled_output()```取出BERT的输出。（BERT网络详见PPT）  \n",
    "第二步：分别取出标准问和扩展问的向量表示  \n",
    "第三步：计算二者的余弦相似度  \n",
    "第四步：将正例部分的余弦相似度值减去1，负例的余弦相似度的值不变  \n",
    "第五步：将第四步的结果取立方，再取绝对值，作为损失函数  \n",
    "\n",
    "\n",
    "经过训练，当模型收敛时，损失函数最大程度趋近于0；其含义分别如下：  \n",
    "负例的余弦相似度在（-1,1）范围内中趋近0，即表征负例中的句子的语义趋近独立不相似，  \n",
    "而正例的相似度在(0,2)范围内趋近于0，即表征正例中的两个句子的语义趋近完全相同  \n",
    "\n",
    "综上，此损失函数对于语义相似的训练是合理且有效的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**代码实现**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 以下为添加的余弦相似度损失函数\n",
    "\n",
    "logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "# 取出标准问和扩展问的向量表示\n",
    "feature_a = logits[:cfg.train_batch_size,:]\n",
    "feature_b = logits[cfg.train_batch_size:,:]\n",
    "\n",
    "# 计算二者的余弦相似度\n",
    "nu = feature_a * feature_b\n",
    "nu = tf.reduce_sum(nu,1)\n",
    "feature_a2 = feature_a * feature_a\n",
    "feature_a2 = tf.reduce_sum(feature_a2,1)\n",
    "feature_a2 = tf.sqrt(feature_a2)\n",
    "feature_b2 = feature_b * feature_b\n",
    "feature_b2 = tf.reduce_sum(feature_b2,1)\n",
    "feature_b2 = tf.sqrt(feature_b2)\n",
    "de = feature_a2 * feature_b2\n",
    "inner_product = nu / de     # 得到余弦相似度\n",
    "\n",
    "\n",
    "target = np.zeros((cfg.train_batch_size),dtype=np.float32)\n",
    "target[:int(cfg.train_batch_size/2)] = 1.0  # train_batch_size中的前一半数据为负例，后一半的数据为正例\n",
    "diffs = inner_product - target  # 正例的余弦相似度减去1，使得范围从(-1,1)变成(-2, 0)，负例的余弦相似度减去的是0，其范围不变\n",
    "diffs = tf.abs(diffs**3)\n",
    "\n",
    "# 当模型收敛时，负例的余弦相似度在（-1,1）范围内中趋近0，即表征负例中的句子含义独立不相似\n",
    "# 而正例的相似度在(0,2)范围内趋近于0，即表征正例中的两个句子含义完全相同\n",
    "loss_op = tf.reduce_mean(diffs)  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.14",
   "language": "python",
   "name": "tf1.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}